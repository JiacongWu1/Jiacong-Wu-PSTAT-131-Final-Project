---
title: "PSTAT 131 Final Project"
author: "Jiacong Wu"
date: "2022-11-22"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

# Introduction

# Preparation

## Loading packages
```{r, message = FALSE, warning = FALSE}
# Loading packages
library(janitor)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(rpart.plot)
library(discrim)
library(ranger)
library(vip)
library(xgboost)
library(ggthemes)
library(neuralnet)
library(caret) 
library(keras)
library(tensorflow)
library(kernlab)
library(yardstick)
tidymodels_prefer()
```
## Loading Data
```{r}
stroke_original = read.csv("data/unprocessed/healthcare-dataset-stroke-data.csv")
```

```{r}
getwd()
```

# Data Cleaning
```{r}
stroke_original = stroke_original%>% 
  select(-id) %>% 
  mutate(gender = factor(gender, levels = c("Male", "Female")),
         heart_disease = factor(heart_disease),
         work_type = factor(work_type),
         ever_married = factor(ever_married),
         smoking_status = factor(smoking_status),
         hypertension = factor(hypertension),
         Residence_type = factor(Residence_type),
         bmi = as.numeric(bmi),
         stroke = factor(stroke),
         age = as.integer(age)) 
stroke = na.omit(stroke_original)
head(stroke)
```

```{r}
write_csv(stroke, file = "data/processed/processed_data.csv")
```


# Data Splitting

```{r}
set.seed(1000)
stroke_split = initial_split(stroke, prop = 0.8, strata = stroke)
train = training(stroke_split)
test = testing(stroke_split)
```

```{r}
print(dim(train))
print(dim(test))
```

# Exportary Data Analysis
```{r Boxplot}
train %>% 
  ggplot() + 
  ggtitle("Box Plot of Glucose Level for Different Work Types")+
  geom_boxplot(aes(x = work_type, y = avg_glucose_level, fill = stroke)) + 
  labs(x = "Work Types", y = "Average Glucose Level") +
  scale_fill_manual(values=c("#78D962", "#FF0000"), labels = c("No", "Yes"))+
  coord_flip()+
  theme(plot.title = element_text(face = "bold.italic", hjust=0.5))
```

```{r, message=FALSE}
train %>% 
  ggplot(aes(x = age, fill = stroke)) + 
  ggtitle("Histogram of People Had Stroke with Their Age")+
  geom_histogram()+
  labs(x = "Age", y = "Number of People")+
  theme(plot.title = element_text(face = "bold.italic", hjust=0.5))+
  scale_fill_manual(values=c("#7CAE00", "#FF0000"), labels = c("No", "Yes"))
```

```{r, message=FALSE}
train %>%
  ggplot(mapping = aes(x = age, y = avg_glucose_level)) + 
  ggtitle("Scatterplot with Trend")+
  geom_point(aes(color = stroke)) +
  geom_smooth(aes(linetype = stroke), se = TRUE)+
  labs(x = "Age", y = "Average Glucose Level")+
  theme(legend.key.size = unit(1.3, 'cm'), 
        plot.title = element_text(face = "bold.italic", hjust=0.5))+
  scale_fill_discrete(labels= c("No", "Yes"))
```

```{r}
train %>%
  ggplot(mapping = aes(x = age, y = bmi)) + 
  ggtitle("BMI and Age")+
  geom_point(aes(color = stroke, shape = stroke)) +
  scale_color_manual(values=c('#97DFFE','#FF0000'))+
  geom_smooth(aes(linetype = stroke), se = TRUE)+
  labs(x = "Age", y = "bmi")+
  theme(legend.key.size = unit(1.3, 'cm'), 
        plot.title = element_text(face = "bold.italic", hjust=0.5))+
  scale_fill_discrete(labels= c("No", "Yes"))
```

```{r}
train %>% 
  ggplot() + 
  ggtitle("Age Distribution of Different Work Types")+
  geom_violin(aes(x = work_type, y = age, fill = stroke)) + 
  labs(x = "Work Types", y = "Age") +
  scale_fill_manual(values=c("#78D962", "#FF0000"), labels = c("No", "Yes"))+
  coord_flip()+
  theme(plot.title = element_text(face = "bold.italic", hjust=0.5))
```

It is easy to see, that the Never_worked people in this data set are all young. Therefore, no one who never worked has a stroke is not because they do not have pressure, it might be just because they are young. 


# Cross Validation
```{r}
stroke_folds <- vfold_cv(train, v = 5, strata = stroke)
stroke_folds
```

# Creating Recipe

```{r}
stroke_recipe = train %>% 
  recipe(stroke ~ gender+
           age+
           hypertension+
           heart_disease+
           ever_married+
           work_type+
           Residence_type+
           avg_glucose_level+
           bmi+
           smoking_status) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```



# Model Selection

## Logistic Regression
```{r}
control <- control_resamples(save_pred = TRUE)
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
log_wkflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(stroke_recipe)
```


```{r}
log_fit <- fit_resamples(log_wkflow, stroke_folds)
```

```{r}
collect_metrics(log_fit)
```


## LDA

```{r}
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")
lda_wkflow <- workflow() %>%
  add_model(lda_mod) %>%
add_recipe(stroke_recipe)
```

```{r}
lda_tune_res <- fit_resamples(resamples = stroke_folds, lda_wkflow, control = control)
```

```{r}
collect_metrics(lda_tune_res)
```

## Desicion Tree

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")
stroke_class_tree_spec <- tree_spec %>%
  set_mode("classification")
```

```{r}
stroke_class_tree_wf <- workflow() %>%
  add_model(stroke_class_tree_spec %>% 
              set_args(cost_complexity = tune())) %>% 
  add_recipe(stroke_recipe)
param_grid_1 <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```


```{r, eval=FALSE}
tune_res <- tune_grid(
  stroke_class_tree_wf, 
  resamples = stroke_folds, 
  grid = param_grid_1, 
  metrics = metric_set(roc_auc)
)
write_rds(tune_res, file = "rds/decision_tree_res.rds")
```

```{r}
decision_tree = read_rds("rds/decision_tree_res.rds")
autoplot(decision_tree)
```

```{r}
best = decision_tree%>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
best
```

This is what the decision tree looks like:

```{r, warning=FALSE}
tree_final <- finalize_workflow(stroke_class_tree_wf, best)
tree_fit <- fit(tree_final,train)
tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

## Random Forest
```{r}
forest_spec = rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification") %>% 
  set_args(mtry = tune(), 
           trees = tune(),
           min_n = tune())
forest_wf = workflow() %>% 
  add_model(forest_spec) %>% 
  add_recipe(stroke_recipe)
```

```{r}
param_grid = grid_regular(mtry(range = c(1,10)), 
                          min_n(range = c(5,20)), 
                          trees(range = c(200,1000)),
                          levels = 10)
```


```{r,eval=FALSE}
rf_tune_res <- tune_grid(
  forest_wf,
  resamples = stroke_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
write_rds(rf_tune_res, file = "rds/rf_res.rds")
```


```{r}
rf = read_rds("rds/rf_res.rds")
autoplot(rf)
```

```{r}
best_rf = rf%>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
best_rf
```

```{r}
rf_final = finalize_workflow(forest_wf,best_rf)
final_fit = fit(rf_final, data = train)
final_fit %>% 
  extract_fit_engine() %>% 
  vip()
```

## Boosted Tree

```{r}
boost = boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
boost_wf = workflow() %>% 
  add_model(boost) %>% 
  add_recipe(stroke_recipe)

boost_grid = grid_regular(trees(c(10,2000)), levels = 10)
```

```{r,eval=FALSE}
boost_tune_res = tune_grid(
  boost_wf,
  resamples = stroke_folds,
  grid = boost_grid,
  metrics = metric_set(roc_auc)
)
write_rds(boost_tune_res, file = "rds/boost_res.rds")
```

```{r}
bst = read_rds("rds/boost_res.rds")
autoplot(bst)
```

```{r}
best_boosting = bst%>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
best_boosting
```

## Support Vector Machine



```{r}
svm_linear_spec <- svm_poly(degree = 1) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```


```{r}
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%
  add_formula(stroke ~ .)

```


```{r}
param_grid <- grid_regular(cost(), levels = 10)
```


```{r, eval=FALSE, warning=FALSE}
svm_tune_res <- tune_grid(
  svm_linear_wf, 
  resamples = stroke_folds, 
  grid = param_grid
)
write_rds(svm_tune_res, file = "rds/svm_res.rds")
```

```{r}
svm = read_rds("rds/svm_res.rds")
autoplot(svm)
```

```{r}
best_svm = best_svm %>% filter(.metric == "accuracy")
best_svm = svm %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  arrange(desc(mean)) %>%
  slice(1)
best_svm
```

## Neural Network
```{r, eval = FALSE}
library(nnet)
nn_wf = workflow() %>% 
  add_model(nnet_fit) %>% 
  add_recipe(stroke_recipe)
stroke_recipe_2 = train %>% 
  recipe(stroke ~ gender+
           age+
           hypertension+
           heart_disease+
           ever_married+
           work_type+
           Residence_type+
           avg_glucose_level+
           bmi+
           smoking_status) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  prep(training = train, retain = TRUE)
train_normalized <- bake(stroke_recipe_2, new_data = train, all_predictors())
nnet_fit <-
  mlp(epochs = 10, hidden_units = 5, dropout = 0.1) %>%
  set_mode("classification") %>% 
  set_engine("nnet", verbose = 0) %>% 
  fit(stroke ~ ., data = bake(stroke_recipe_2, new_data = NULL))
nnet_fit
```

```{r, eval = FALSE}
nn_res<- predict(train, nnet_fit)
```

# Best Model Fitting and Testing

# Conclusion

# Discussion

# Reference
